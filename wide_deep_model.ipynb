{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wide_deep_model.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNrEZUAGCk94tkQ1wlPGMnU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlskawns/cp2/blob/main/wide_deep_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install category_encoders\n",
        "!pip install kmodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b-Rm7O9Ta23",
        "outputId": "7742a739-2983-492e-dbb2-fa086d1a4fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.3.0-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[?25l\r\u001b[K     |████                            | 10 kB 35.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 20 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 30 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 40 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 51 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 61 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 71 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 81 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 82 kB 430 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.0.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.5.2)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.10.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.21.5)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.1.0)\n",
            "Installing collected packages: category-encoders\n",
            "Successfully installed category-encoders-2.3.0\n",
            "Collecting kmodes\n",
            "  Downloading kmodes-0.11.1-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from kmodes) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from kmodes) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from kmodes) (1.0.2)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from kmodes) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.0->kmodes) (3.1.0)\n",
            "Installing collected packages: kmodes\n",
            "Successfully installed kmodes-0.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvkDNySYSUuB",
        "outputId": "3c973b5e-aab8-4010-f74c-debe10fadee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2, l1_l2\n",
        "\n",
        "from category_encoders import OrdinalEncoder\n",
        "from kmodes.kprototypes import KPrototypes\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "import pickle\n",
        "import copy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y44XN_jWTnBd",
        "outputId": "9507e2f7-dc8d-4100-aa3d-ef9f1927fb54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정책데이터셋 클러스터링 및 유저데이터와의 결합 후 라벨 지정\n",
        "with open('/content/drive/MyDrive/웰로/data/user_policy_clustered.pkl' , 'rb') as f:\n",
        "  l = pickle.load(f)"
      ],
      "metadata": {
        "id": "w5DSBZKMTguO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class recommendation_preprocessing:\n",
        "    \"\"\"\n",
        "    추천 진행을 위한 클래스\n",
        "    특정 유저 number 기입시, 해당 유저의 \n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "      \n",
        "\n",
        "      self.policy_data = pd.read_csv('/content/drive/MyDrive/웰로/data/policy_data.csv', encoding = 'utf-8')  # 정책데이터 룩업테이블\n",
        "      self.policy_data.rename(columns = {i:i+'_y' if i not in ['정책ID', '서비스명', '대상연령시작','대상연령끝','소관기관유형','지원유형','자녀','농축수산인','보육지원(0~7세)', '교육지원(8~19세)'] else i for i in self.policy_data.columns }, inplace=True)\n",
        "      # 특정유저 데이터 전처리\n",
        "    def user_policy_preprocessing(self, user): \n",
        "      self.class_to_columns_general(user, 'mb_10+대상특성')\n",
        "      self.income_change_user_general(user)\n",
        "      user[['mb_15']].fillna('관심정책없음',inplace=True)\n",
        "      self.class_to_columns_general(user, 'mb_15')\n",
        "      user  = pd.DataFrame(user).T.drop(columns = ['시군구','자녀상세','자녀수','자녀','mb_10','mb_11','mb_10+대상특성', 'mb_11+관심상황특성','mb_12','mb_13','mb_14','mb_15'])\n",
        "      user['결혼'] = user['결혼'].replace('미혼,기혼','무관')\n",
        "      a = user.columns\n",
        "      # 유저 정책 데이터 병합\n",
        "      for i in a:\n",
        "        self.policy_data[i] = user[i].values[0]\n",
        "      self.policy_data = self.policy_data.rename(columns={'성별':'성별_x','학력':'학력_x','결혼':'결혼_x','농축수산인_x':' 농축수산인','교육지원(만8~19세)_x':'교육지원(만8~19세)','max_income':'max_income_x',\n",
        "                                                  'min_income':'min_income_x','보육지원(만0~7세)_x':'보육지원(만0~7세)','관심정책없음_x':'관심정책없음','시도':'시도_x','직장':'직장_x'}) #'시도':'시도_x','직장':'직장_x'\n",
        "\n",
        "      # self.policy_data = self.policy_data[cols]\n",
        "\n",
        "      return self.policy_data\n",
        "    def income_change_user_general(self, user):\n",
        "      if user['mb_12'] != None:\n",
        "        if '이하' in user['mb_12']:\n",
        "          user['max_income'] = 40\n",
        "          user['min_income'] = 0\n",
        "        elif '이상' in user['mb_12']:\n",
        "          user['max_income'] = 110     # 100이상은 모두 포함\n",
        "          user['min_income'] = 100\n",
        "        elif '기초생활수급자' in user['mb_12']:\n",
        "          user['max_income'] = 30\n",
        "          user['min_income'] = 0\n",
        "        elif '사이' in user['mb_12']:\n",
        "          if '40~60' in user['mb_12']:\n",
        "            user['max_income'] = 60\n",
        "            user['min_income'] = 40\n",
        "          elif '60~80' in user['mb_12']:\n",
        "            user['max_income'] = 80\n",
        "            user['min_income'] = 60\n",
        "          elif '80~100' in user['mb_12']:\n",
        "            user['max_income'] = 100\n",
        "            user['min_income'] = 80\n",
        "          else: # 40% 이하\n",
        "            user['max_income'] = 40\n",
        "            user['min_income'] = 0\n",
        "        else:\n",
        "          user['max_income'] = 110     # 100이상은 모두 포함\n",
        "          user['min_income'] = 0\n",
        "      else:\n",
        "        user['max_income'] = 110     \n",
        "        user['min_income'] = 0\n",
        "\n",
        "\n",
        "    def class_to_columns_general(self, user, col):\n",
        "        \"\"\"\n",
        "        유저 추천을 위한 전처리 함수\n",
        "        \"\"\"\n",
        "        b = []\n",
        "        a = user[col]\n",
        "\n",
        "        if col == 'mb_10+대상특성':\n",
        "          mb_10_cols = ['다자녀가정', '농축수산인',\t'장애인',\t'질병/부상/질환자',\t'북한이탈주민',\t'한부모가정/조손가정',\t'해당사항없음',\t'국가유공자',\t'다문화가족']\n",
        "          if a != None:\n",
        "            if ',' in a:\n",
        "              a = a.split(',')\n",
        "              b.extend(a)\n",
        "            else:\n",
        "              b.append(a) \n",
        "            for i in mb_10_cols:\n",
        "              if i in b:\n",
        "                user[i+'_x'] = 1\n",
        "              else:\n",
        "                user[i+'_x'] = 0\n",
        "          elif a == None:\n",
        "            for i in mb_10_cols:\n",
        "              if i in '해당사항없음':\n",
        "                user[i+'_x'] = 1\n",
        "              else:\n",
        "                user[i+'_x'] = 0\n",
        "        elif col == 'mb_15':\n",
        "          mb_15_cols = ['근로자 지원',\t'교육지원(만8~19세)',\t'취업 지원',\t'문화생활 지원',\t'의료 지원',\t'보육지원(만0~7세)',\t'관심정책없음',\t'기업금융지원',\t'주택-부동산 지원',\t'개인금융지원',\t'성인교육지원',\t'창업 지원']\n",
        "          if a != None:\n",
        "            if ',' in a:\n",
        "              a = a.split(',')\n",
        "              b.extend(a)\n",
        "            else:\n",
        "              b.append(a) \n",
        "            for i in mb_15_cols:\n",
        "              if i in b:\n",
        "                user[i+'_x'] = 1\n",
        "              else:\n",
        "                user[i+'_x'] = 0\n",
        "                \n",
        "          elif a == None:\n",
        "            for i in mb_15_cols:\n",
        "              if i in '관심정책없음':\n",
        "                user[i+'_x'] = 1\n",
        "              else:\n",
        "                user[i+'_x'] = 0\n",
        "          user = pd.DataFrame(user).T.rename({'\\t농축수산인_x': ' 농축수산인'})\n",
        "\n"
      ],
      "metadata": {
        "id": "qvifXhx8N3-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Data:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.utils = Utils()\n",
        "        self.predict_features = None\n",
        "        # self.categorical_columns = None\n",
        "        self.crossed_columns_dic = None\n",
        "    def get_wide_model_data(self, df_train, df_test= None, prediction = None):\n",
        "        # x : 유저 / y: 정책\n",
        "        # 중요 feature인 대상특성과 관심정책(mb_15) 관련 \n",
        "        wide_cols = [ '성별_x', '결혼_x',\n",
        "              ' 농축수산인', '해당사항없음_x', '한부모가정/조손가정_x', '국가유공자_x', '북한이탈주민_x',\n",
        "              '질병/부상/질환자_x', '장애인_x', '다문화가족_x', '다자녀가정_x', '문화생활 지원_x', '주택-부동산 지원_x', '관심정책없음', '근로자 지원_x',\n",
        "              '의료 지원_x', '보육지원(만0~7세)', '개인금융지원_x', '교육지원(만8~19세)', '성인교육지원_x',\n",
        "              '기업금융지원_x', '취업 지원_x', '창업 지원_x',\n",
        "              '성별_y', '결혼_y', '질병/부상/질환자_y', '국가유공자_y', '한부모가정/조손가정_y', '다자녀가정_y', '해당사항없음_y', '다문화가족_y', '북한이탈주민_y', '농축수산인', '장애인_y',\n",
        "              '개인금융지원_y', '문화생활 지원_y', '취업 지원_y', '창업 지원_y', '교육지원(8~19세)','의료 지원_y', '보육지원(0~7세)', '주택-부동산 지원_y', '성인교육지원_y', '기업금융지원_y','근로자 지원_y']\n",
        "\n",
        "        crossed_cols = (['성별_x', '성별_y'], ['결혼_x', '결혼_y']) #,['직장_x','직장_y'],['학력_x','학력_y'],['시도_x', '시도_y'])\n",
        "        target = 'label'\n",
        "       \n",
        "        \n",
        "        if prediction:\n",
        "          df_wide = df_train\n",
        "          for col_name, col_lst in self.crossed_columns_dic.items():\n",
        "            df_wide[col_name] = df_wide[col_lst].apply(lambda x: '-'.join(x), axis=1)\n",
        "          print('df_wide cols:',df_wide.columns)\n",
        "          # dummy로 변경\n",
        "\n",
        "          df_wide = pd.get_dummies(df_wide, columns=self.dummy_cols)\n",
        "\n",
        "          d_lst = [x for x in df_wide.columns if x not in self.predict_features]\n",
        "          df_wide = df_wide.drop(columns = d_lst)\n",
        "\n",
        "          add_lst = [x for x in self.predict_features if x not in df_wide.columns]\n",
        "          for i in add_lst:\n",
        "            df_wide[i] = 0\n",
        "\n",
        "\n",
        "          X_predict = np.array(df_wide[self.predict_features].values, dtype = np.float)\n",
        "\n",
        "          return X_predict\n",
        "        elif prediction == None:\n",
        "          df_train['IS_TRAIN'] = 1\n",
        "          df_test['IS_TRAIN'] = 0\n",
        "          df_wide = pd.concat([df_train, df_test])\n",
        "\n",
        "          categorical_columns = list(df_wide.select_dtypes(include=['object']).columns)\n",
        "          self.crossed_columns_dic = self.utils.cross_columns(crossed_cols)\n",
        "          wide_cols += list(self.crossed_columns_dic.keys())\n",
        "\n",
        "          # crossed 변수 추가\n",
        "          for col_name, col_lst in self.crossed_columns_dic.items():\n",
        "              df_wide[col_name] = df_wide[col_lst].apply(lambda x: '-'.join(x), axis=1)\n",
        "          df_wide = df_wide[wide_cols + [target] + ['IS_TRAIN']]\n",
        "\n",
        "          # dummy로 변경\n",
        "          self.dummy_cols = [wc for wc in wide_cols if wc in categorical_columns + list(self.crossed_columns_dic.keys())]\n",
        "          df_wide = pd.get_dummies(df_wide, columns=self.dummy_cols)\n",
        "          train = df_wide[df_wide.IS_TRAIN == 1].drop('IS_TRAIN', axis=1)\n",
        "          test = df_wide[df_wide.IS_TRAIN == 0].drop('IS_TRAIN', axis=1)\n",
        "\n",
        "          self.predict_features = [c for c in train.columns if c != target]\n",
        "\n",
        "          X_train = np.array(train[self.predict_features].values, dtype = np.float)\n",
        "          y_train = train['label'].values.reshape(-1, 1)\n",
        "\n",
        "          X_test = np.array(test[self.predict_features].values, dtype = np.float)\n",
        "          y_test = test['label'].values.reshape(-1, 1)\n",
        "          return X_train, y_train, X_test, y_test\n",
        "\n",
        "        \n",
        "    def get_deep_model_data(self, df_train, df_test=None, prediction = None):\n",
        "\n",
        "        embedding_cols = ['시도_x', '학력_x', '직장_x', '결혼_x',\n",
        "              ' 농축수산인', '해당사항없음_x', '한부모가정/조손가정_x', '국가유공자_x', '북한이탈주민_x','질병/부상/질환자_x', '장애인_x', '다문화가족_x', '다자녀가정_x',\n",
        "              '문화생활 지원_x', '주택-부동산 지원_x', '관심정책없음', '근로자 지원_x','의료 지원_x', '보육지원(만0~7세)', '개인금융지원_x', '교육지원(만8~19세)', '성인교육지원_x','기업금융지원_x', '취업 지원_x', '창업 지원_x',\n",
        "              '시도_y', '소관기관유형', '지원유형', '학력_y', '성별_y', '결혼_y', '자녀', '직장_y',\n",
        "              '농축수산인', '해당사항없음_y','한부모가정/조손가정_y', '국가유공자_y', '북한이탈주민_y', '질병/부상/질환자_y', '장애인_y', '다문화가족_y', '다자녀가정_y', \n",
        "              '개인금융지원_y', '문화생활 지원_y', '취업 지원_y', '창업 지원_y', '교육지원(8~19세)','의료 지원_y', '보육지원(0~7세)', '주택-부동산 지원_y', '성인교육지원_y', '기업금융지원_y','근로자 지원_y']\n",
        "        cont_cols = ['나이','max_income_x','min_income_x','대상연령시작','대상연령끝','max_income_y', 'min_income_y'] \n",
        "        target = 'label'\n",
        "        if prediction:\n",
        "          df_deep = df_train\n",
        "          deep_cols = embedding_cols + cont_cols\n",
        "          df_deep = df_deep[deep_cols]\n",
        "\n",
        "\n",
        "          scaler = StandardScaler()\n",
        "          df_deep[cont_cols] = pd.DataFrame(scaler.fit_transform(df_train[cont_cols]), columns=cont_cols)\n",
        "          df_deep, col_to_unique_val_num = self.utils.val2idx(df_deep, embedding_cols)\n",
        "\n",
        "          embeddings_tensors = []\n",
        "          for ec in embedding_cols:\n",
        "              layer_name = ec + '_inp'\n",
        "              inp = Input(shape=(1,), dtype='int64', name=layer_name)\n",
        "              embd = Embedding(col_to_unique_val_num[ec], 8, input_length=1, embeddings_regularizer=l2(1e-3))(inp)\n",
        "              embeddings_tensors.append((inp, embd))\n",
        "              del (inp, embd)\n",
        "\n",
        "          continuous_tensors = []\n",
        "          for cc in cont_cols:\n",
        "              layer_name = cc + '_in'\n",
        "              inp = Input(shape=(1,), dtype='float32', name=layer_name)\n",
        "              bulid = Reshape((1, 1))(inp)\n",
        "              continuous_tensors.append((inp, bulid))\n",
        "              del (inp, bulid)\n",
        "          X = [df_deep[c] for c in deep_cols]\n",
        "          return X, embeddings_tensors, continuous_tensors\n",
        "\n",
        "        elif prediction == None:\n",
        "          df_train['IS_TRAIN'] = 1\n",
        "          df_test['IS_TRAIN'] = 0\n",
        "          df_deep = pd.concat([df_train, df_test])\n",
        "          deep_cols = embedding_cols + cont_cols\n",
        "          df_deep = df_deep[deep_cols + [target, 'IS_TRAIN']]\n",
        "\n",
        "          scaler = StandardScaler()\n",
        "          df_deep[cont_cols] = pd.DataFrame(scaler.fit_transform(df_train[cont_cols]), columns=cont_cols)\n",
        "          df_deep, col_to_unique_val_num = self.utils.val2idx(df_deep, embedding_cols)\n",
        "  \n",
        "          df_deep.fillna(-1, inplace=True)\n",
        "\n",
        "\n",
        "          train = df_deep[df_deep.IS_TRAIN == 1].drop('IS_TRAIN', axis=1)\n",
        "          test = df_deep[df_deep.IS_TRAIN == 0].drop('IS_TRAIN', axis=1)\n",
        "\n",
        "          embeddings_tensors = []\n",
        "          for ec in embedding_cols:\n",
        "              layer_name = ec + '_inp'\n",
        "              inp = Input(shape=(1,), dtype='int64', name=layer_name)\n",
        "              embd = Embedding(col_to_unique_val_num[ec], 8, input_length=1, embeddings_regularizer=l2(1e-3))(inp)\n",
        "              embeddings_tensors.append((inp, embd))\n",
        "              del (inp, embd)\n",
        "\n",
        "          continuous_tensors = []\n",
        "          for cc in cont_cols:\n",
        "              layer_name = cc + '_in'\n",
        "              inp = Input(shape=(1,), dtype='float32', name=layer_name)\n",
        "              bulid = Reshape((1, 1))(inp)\n",
        "              continuous_tensors.append((inp, bulid))\n",
        "              del (inp, bulid)\n",
        "\n",
        "          X_train = [train[c] for c in deep_cols]\n",
        "          y_train = np.array(train[target].values).reshape(-1, 1)\n",
        "\n",
        "          X_test = [test[c] for c in deep_cols]\n",
        "          y_test = np.array(test[target].values).reshape(-1, 1)\n",
        "\n",
        "          return X_train, y_train, X_test, y_test, embeddings_tensors, continuous_tensors\n",
        "\n",
        "class Utils:\n",
        "\n",
        "    def cross_columns(self, x_cols):\n",
        "\n",
        "        crossed_columns = dict()\n",
        "        colnames = ['_'.join(x_c) for x_c in x_cols]\n",
        "        for cname, x_c in zip(colnames, x_cols):\n",
        "            crossed_columns[cname] = x_c\n",
        "        return crossed_columns\n",
        "\n",
        "    def val2idx(self, df, cols):\n",
        "\n",
        "        val_types = dict()\n",
        "        for c in cols:\n",
        "            val_types[c] = df[c].unique()\n",
        "\n",
        "        val_to_idx = dict()\n",
        "        for k, v in val_types.items():\n",
        "            val_to_idx[k] = {o: i for i, o in enumerate(val_types[k])}\n",
        "\n",
        "        for k, v in val_to_idx.items():\n",
        "            df[k] = df[k].apply(lambda x: v[x])\n",
        "\n",
        "        unique_vals = dict()  # 사용한 값만\n",
        "        for c in cols:\n",
        "            unique_vals[c] = df[c].nunique()\n",
        "\n",
        "        return df, unique_vals"
      ],
      "metadata": {
        "id": "hJbPOb5_N4Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Wide_Deep:\n",
        "\n",
        "    def __init__(self, X_train_wide, y_train_wide,\n",
        "                       X_train_deep, y_train_deep,\n",
        "                       embeddings_tensors, continuous_tensors):\n",
        "        # WIDE\n",
        "        wide_input = Input(shape=(X_train_wide.shape[1],), dtype='float32', name='wide')\n",
        "        # DEEP\n",
        "        deep_input = [et[0] for et in embeddings_tensors] + [ct[0] for ct in continuous_tensors]\n",
        "        deep_embedding = [et[1] for et in embeddings_tensors] + [ct[1] for ct in continuous_tensors]\n",
        "\n",
        "        deep_embedding = Flatten()(concatenate(deep_embedding))\n",
        "        layer_1 = Dense(50, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(deep_embedding)\n",
        "        layer_1_dropout = Dropout(0.5)(layer_1)\n",
        "        layer_2 = Dense(20, activation='relu', name='deep')(layer_1_dropout)\n",
        "        layer_2_dropout = Dropout(0.5)(layer_2)\n",
        "\n",
        "        # WIDE & DEEP\n",
        "        wd_input = concatenate([wide_input, layer_2_dropout])\n",
        "        wd_output = Dense(1, activation='sigmoid', name='wide_deep')(wd_input)\n",
        "\n",
        "        self.wide_deep_model = Model([wide_input, deep_input], wd_output)\n",
        "        self.wide_deep_model.compile(tf.keras.optimizers.Adam(learning_rate = 0.01, decay = 0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.wide_deep_model\n",
        "\n",
        "\n",
        "class Run:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        l_2 = l.copy()\n",
        "        l_2.max_income_y = l_2.max_income_y.astype('int64')\n",
        "        l_2.min_income_y = l_2.min_income_y.astype('int64')\n",
        "        l_2.label = l_2.label.astype('int64')\n",
        "        self.df_train, self.df_test = train_test_split(l_2, test_size = 0.2, random_state = 2)\n",
        "\n",
        "        target = 'label'\n",
        "        # self.model = None\n",
        "        self.load = Data()\n",
        "\n",
        "\n",
        "    def Wide_and_Deep(self):\n",
        "\n",
        "        X_train_wide, y_train_wide, X_test_wide, y_test_wide = self.load.get_wide_model_data(self.df_train, self.df_test)\n",
        "        X_train_deep, y_train_deep, X_test_deep, y_test_deep, \\\n",
        "        embeddings_tensors, continuous_tensors = self.load.get_deep_model_data(self.df_train, self.df_test)\n",
        "\n",
        "        X_tr_wd = [X_train_wide] + X_train_deep\n",
        "        y_tr_wd = y_train_deep\n",
        "\n",
        "        X_te_wd = [X_test_wide] + X_test_deep\n",
        "        y_te_wd = y_test_deep\n",
        "\n",
        "        model = Wide_Deep(X_train_wide, y_train_wide, X_train_deep, y_train_deep, embeddings_tensors, continuous_tensors)\n",
        "        early = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 3)\n",
        "        self.model = model.get_model()\n",
        "        self.model.fit(X_tr_wd, y_tr_wd, epochs=10, batch_size=128, validation_data = [X_te_wd, y_te_wd])\n",
        "\n",
        "        print('wide and deep model accuracy:', self.model.evaluate(X_te_wd, y_te_wd)[1])\n",
        "        \n",
        "        eval_pred = self.model.predict(X_te_wd)\n",
        "        eval_pred = eval_pred >= 0.5\n",
        "        \n",
        "        print(classification_report(y_te_wd, eval_pred))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def Recommendation(self, user):\n",
        "        rec = recommendation_preprocessing()\n",
        "        user = rec.user_policy_preprocessing(user)\n",
        "        \n",
        "        x_wide =  self.load.get_wide_model_data(user, prediction=True)\n",
        "        x_deep, embeddings_tensors, continuous_tensors = self.load.get_deep_model_data(user, prediction = True)\n",
        "        x_input = [x_wide]+x_deep\n",
        "        pred = self.model.predict(x_input)\n",
        "        user['prob'] = pred\n",
        "        user = user.sort_values(by = 'prob', ascending = False).reset_index()\n",
        "\n",
        "        \n",
        "\n",
        "        return user[['정책ID','서비스명','prob']].head(10)\n",
        "\n"
      ],
      "metadata": {
        "id": "n7kBddrFahoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = Run()\n",
        "run.Wide_and_Deep()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7ed6SYkAq4r",
        "outputId": "f81dc9da-e053-410f-c7e7-9f3dd447536b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "18750/18750 [==============================] - 184s 10ms/step - loss: 0.4793 - accuracy: 0.8713 - val_loss: 0.4127 - val_accuracy: 0.8731\n",
            "Epoch 2/10\n",
            "18750/18750 [==============================] - 182s 10ms/step - loss: 0.3896 - accuracy: 0.8730 - val_loss: 0.3740 - val_accuracy: 0.8736\n",
            "Epoch 3/10\n",
            "18750/18750 [==============================] - 183s 10ms/step - loss: 0.3641 - accuracy: 0.8730 - val_loss: 0.3583 - val_accuracy: 0.8727\n",
            "Epoch 4/10\n",
            "18750/18750 [==============================] - 182s 10ms/step - loss: 0.3516 - accuracy: 0.8731 - val_loss: 0.3478 - val_accuracy: 0.8741\n",
            "Epoch 5/10\n",
            "18750/18750 [==============================] - 184s 10ms/step - loss: 0.3441 - accuracy: 0.8733 - val_loss: 0.3422 - val_accuracy: 0.8732\n",
            "Epoch 6/10\n",
            "18750/18750 [==============================] - 183s 10ms/step - loss: 0.3391 - accuracy: 0.8734 - val_loss: 0.3372 - val_accuracy: 0.8718\n",
            "Epoch 7/10\n",
            "18750/18750 [==============================] - 186s 10ms/step - loss: 0.3356 - accuracy: 0.8732 - val_loss: 0.3348 - val_accuracy: 0.8745\n",
            "Epoch 8/10\n",
            "18750/18750 [==============================] - 188s 10ms/step - loss: 0.3329 - accuracy: 0.8734 - val_loss: 0.3321 - val_accuracy: 0.8723\n",
            "Epoch 9/10\n",
            "18750/18750 [==============================] - 189s 10ms/step - loss: 0.3308 - accuracy: 0.8734 - val_loss: 0.3307 - val_accuracy: 0.8719\n",
            "Epoch 10/10\n",
            "18750/18750 [==============================] - 193s 10ms/step - loss: 0.3291 - accuracy: 0.8734 - val_loss: 0.3289 - val_accuracy: 0.8740\n",
            "18750/18750 [==============================] - 184s 10ms/step - loss: 0.3289 - accuracy: 0.8740\n",
            "wide and deep model accuracy: 0.873960018157959\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.88      0.89    342435\n",
            "           1       0.85      0.86      0.85    257565\n",
            "\n",
            "    accuracy                           0.87    600000\n",
            "   macro avg       0.87      0.87      0.87    600000\n",
            "weighted avg       0.87      0.87      0.87    600000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ggnfOiJuZY6R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}